{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Salt Crystal Purity Classification - MobileNetV2\n",
        "\n",
        "This notebook trains a **MobileNetV2** model to classify salt crystals as **pure** or **impure** using transfer learning.\n",
        "\n",
        "## Model Architecture: MobileNetV2\n",
        "\n",
        "MobileNetV2 is a lightweight convolutional neural network designed for mobile and embedded vision applications.\n",
        "\n",
        "### Key Features:\n",
        "- **Inverted Residual Blocks**: Expand-depthwise-project structure\n",
        "- **Linear Bottlenecks**: Prevents information loss in narrow layers\n",
        "- **Depthwise Separable Convolutions**: Reduces computation by 8-9x\n",
        "- **Parameters**: ~3.4 million (vs 25M for ResNet50)\n",
        "- **Input Size**: 224x224x3\n",
        "\n",
        "### Comparison Context\n",
        "This model is trained for **image classification** (one label per image) to compare against **YOLOv8 object detection** which provides:\n",
        "- Bounding box localization\n",
        "- Multiple object detection per image\n",
        "- Per-crystal confidence scores\n",
        "\n",
        "## Before Starting\n",
        "1. Go to **Runtime > Change runtime type**\n",
        "2. Select **T4 GPU** (or any available GPU)\n",
        "3. Click **Save**"
      ],
      "metadata": {
        "id": "mobilenet_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 1: Check GPU & Install Dependencies"
      ],
      "metadata": {
        "id": "step1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import tensorflow as tf\n",
        "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/upgrade required packages\n",
        "!pip install -q pillow scikit-learn matplotlib seaborn\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Dependencies installed successfully!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 2: Mount Google Drive & Load Dataset\n",
        "\n",
        "The dataset is stored in Google Drive at `MyDrive/salt-crystal/data.zip` (same as YOLOv8 training)."
      ],
      "metadata": {
        "id": "step2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your dataset in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/salt-crystal/data.zip'\n",
        "\n",
        "# Verify the file exists\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"\\nDataset found: {zip_path}\")\n",
        "else:\n",
        "    print(f\"\\nERROR: Dataset not found at {zip_path}\")\n",
        "    print(\"Please check the path and try again.\")\n",
        "\n",
        "# Extract the dataset\n",
        "print(\"\\nExtracting dataset...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset_yolo')\n",
        "\n",
        "print(\"Dataset extracted successfully!\")\n",
        "print(\"\\nExtracted contents:\")\n",
        "!ls -la /content/dataset_yolo"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 3: Convert YOLO Detection Format to Classification Format\n",
        "\n",
        "### Problem:\n",
        "- YOLO format: Images with bounding box annotations (multiple crystals per image)\n",
        "- Classification format: Individual crystal images in class folders\n",
        "\n",
        "### Solution:\n",
        "Crop each annotated crystal from the source images and organize into `pure/` and `impure/` folders."
      ],
      "metadata": {
        "id": "step3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "# Source paths (YOLO format)\n",
        "SOURCE_IMAGES = '/content/dataset_yolo/images'\n",
        "SOURCE_LABELS = '/content/dataset_yolo/labels'\n",
        "CLASSES_FILE = '/content/dataset_yolo/classes.txt'\n",
        "\n",
        "# Target paths (Classification format)\n",
        "TARGET_DIR = '/content/dataset_classification'\n",
        "\n",
        "# Read class names\n",
        "with open(CLASSES_FILE, 'r') as f:\n",
        "    classes = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "print(f\"Classes: {classes}\")\n",
        "print(f\"Class 0: {classes[0]}\")\n",
        "print(f\"Class 1: {classes[1]}\")\n",
        "\n",
        "# Create target directories\n",
        "for split in ['train', 'valid']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(f'{TARGET_DIR}/{split}/{cls}', exist_ok=True)\n",
        "\n",
        "print(f\"\\nTarget directory structure created at {TARGET_DIR}\")"
      ],
      "metadata": {
        "id": "setup_dirs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_crystals_from_yolo(image_dir, label_dir, output_dir, classes, target_size=224):\n",
        "    \"\"\"\n",
        "    Crop individual crystals from images using YOLO annotations.\n",
        "    \n",
        "    YOLO format: class_id x_center y_center width height (normalized 0-1)\n",
        "    \"\"\"\n",
        "    stats = {cls: 0 for cls in classes}\n",
        "    \n",
        "    image_files = [f for f in os.listdir(image_dir) \n",
        "                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "    \n",
        "    for img_file in image_files:\n",
        "        # Load image\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_width, img_height = img.size\n",
        "        \n",
        "        # Find corresponding label file\n",
        "        label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        \n",
        "        if not os.path.exists(label_path):\n",
        "            continue\n",
        "        \n",
        "        # Read annotations\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        for idx, line in enumerate(lines):\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5:\n",
        "                continue\n",
        "            \n",
        "            class_id = int(parts[0])\n",
        "            x_center = float(parts[1]) * img_width\n",
        "            y_center = float(parts[2]) * img_height\n",
        "            width = float(parts[3]) * img_width\n",
        "            height = float(parts[4]) * img_height\n",
        "            \n",
        "            # Calculate bounding box coordinates\n",
        "            x1 = max(0, int(x_center - width / 2))\n",
        "            y1 = max(0, int(y_center - height / 2))\n",
        "            x2 = min(img_width, int(x_center + width / 2))\n",
        "            y2 = min(img_height, int(y_center + height / 2))\n",
        "            \n",
        "            # Skip very small crops\n",
        "            if (x2 - x1) < 10 or (y2 - y1) < 10:\n",
        "                continue\n",
        "            \n",
        "            # Crop and resize\n",
        "            crop = img.crop((x1, y1, x2, y2))\n",
        "            crop = crop.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
        "            \n",
        "            # Save cropped crystal\n",
        "            class_name = classes[class_id]\n",
        "            output_filename = f\"{os.path.splitext(img_file)[0]}_crop{idx}.jpg\"\n",
        "            output_path = os.path.join(output_dir, class_name, output_filename)\n",
        "            crop.save(output_path, 'JPEG', quality=95)\n",
        "            \n",
        "            stats[class_name] += 1\n",
        "    \n",
        "    return stats\n",
        "\n",
        "print(\"Crystal cropping function defined.\")"
      ],
      "metadata": {
        "id": "crop_function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's do a train/valid split of the source images\n",
        "image_files = [f for f in os.listdir(SOURCE_IMAGES) \n",
        "               if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "random.seed(42)  # Same seed as YOLOv8 for consistency\n",
        "random.shuffle(image_files)\n",
        "\n",
        "split_idx = int(len(image_files) * 0.9)\n",
        "train_files = set(image_files[:split_idx])\n",
        "valid_files = set(image_files[split_idx:])\n",
        "\n",
        "print(f\"Total images: {len(image_files)}\")\n",
        "print(f\"Train images: {len(train_files)}\")\n",
        "print(f\"Valid images: {len(valid_files)}\")\n",
        "\n",
        "# Create temporary directories for split\n",
        "os.makedirs('/content/temp_train/images', exist_ok=True)\n",
        "os.makedirs('/content/temp_train/labels', exist_ok=True)\n",
        "os.makedirs('/content/temp_valid/images', exist_ok=True)\n",
        "os.makedirs('/content/temp_valid/labels', exist_ok=True)\n",
        "\n",
        "# Copy files to temp directories\n",
        "for img_file in train_files:\n",
        "    shutil.copy(os.path.join(SOURCE_IMAGES, img_file), '/content/temp_train/images/')\n",
        "    label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "    label_path = os.path.join(SOURCE_LABELS, label_file)\n",
        "    if os.path.exists(label_path):\n",
        "        shutil.copy(label_path, '/content/temp_train/labels/')\n",
        "\n",
        "for img_file in valid_files:\n",
        "    shutil.copy(os.path.join(SOURCE_IMAGES, img_file), '/content/temp_valid/images/')\n",
        "    label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "    label_path = os.path.join(SOURCE_LABELS, label_file)\n",
        "    if os.path.exists(label_path):\n",
        "        shutil.copy(label_path, '/content/temp_valid/labels/')\n",
        "\n",
        "print(\"\\nFiles split into train/valid directories.\")"
      ],
      "metadata": {
        "id": "split_files"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crop crystals for training set\n",
        "print(\"Cropping training crystals...\")\n",
        "train_stats = crop_crystals_from_yolo(\n",
        "    '/content/temp_train/images',\n",
        "    '/content/temp_train/labels',\n",
        "    f'{TARGET_DIR}/train',\n",
        "    classes\n",
        ")\n",
        "print(f\"Training set: {train_stats}\")\n",
        "\n",
        "# Crop crystals for validation set\n",
        "print(\"\\nCropping validation crystals...\")\n",
        "valid_stats = crop_crystals_from_yolo(\n",
        "    '/content/temp_valid/images',\n",
        "    '/content/temp_valid/labels',\n",
        "    f'{TARGET_DIR}/valid',\n",
        "    classes\n",
        ")\n",
        "print(f\"Validation set: {valid_stats}\")\n",
        "\n",
        "# Clean up temp directories\n",
        "shutil.rmtree('/content/temp_train')\n",
        "shutil.rmtree('/content/temp_valid')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATASET PREPARATION COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nTraining samples:\")\n",
        "for cls in classes:\n",
        "    count = len(os.listdir(f'{TARGET_DIR}/train/{cls}'))\n",
        "    print(f\"  {cls}: {count} images\")\n",
        "\n",
        "print(f\"\\nValidation samples:\")\n",
        "for cls in classes:\n",
        "    count = len(os.listdir(f'{TARGET_DIR}/valid/{cls}'))\n",
        "    print(f\"  {cls}: {count} images\")"
      ],
      "metadata": {
        "id": "crop_crystals"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize some cropped samples\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "\n",
        "for row, cls in enumerate(classes):\n",
        "    cls_dir = f'{TARGET_DIR}/train/{cls}'\n",
        "    samples = os.listdir(cls_dir)[:5]\n",
        "    \n",
        "    for col, sample in enumerate(samples):\n",
        "        img = Image.open(os.path.join(cls_dir, sample))\n",
        "        axes[row, col].imshow(img)\n",
        "        axes[row, col].set_title(f'{cls}')\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "plt.suptitle('Sample Cropped Crystals (224x224)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "visualize_samples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 4: Create Data Generators with Augmentation"
      ],
      "metadata": {
        "id": "step4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Training data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation data (no augmentation, only rescaling)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create generators\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{TARGET_DIR}/train',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    f'{TARGET_DIR}/valid',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"\\nClass indices: {train_generator.class_indices}\")\n",
        "print(f\"Training samples: {train_generator.samples}\")\n",
        "print(f\"Validation samples: {valid_generator.samples}\")"
      ],
      "metadata": {
        "id": "data_generators"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 5: Build MobileNetV2 Model with Transfer Learning"
      ],
      "metadata": {
        "id": "step5_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load MobileNetV2 with pretrained ImageNet weights (without top layer)\n",
        "base_model = MobileNetV2(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "\n",
        "# Freeze base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the complete model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nMobileNetV2 Model Architecture:\")\n",
        "print(\"=\"*50)\n",
        "model.summary()\n",
        "\n",
        "# Count parameters\n",
        "total_params = model.count_params()\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "non_trainable_params = total_params - trainable_params\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Non-trainable parameters: {non_trainable_params:,}\")"
      ],
      "metadata": {
        "id": "build_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 6: Training Phase 1 - Frozen Base (Feature Extraction)"
      ],
      "metadata": {
        "id": "step6_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        '/content/mobilenet_best.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"Training Phase 1: Feature Extraction (Frozen Base)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train with frozen base\n",
        "history_frozen = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=valid_generator,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nPhase 1 Training Complete!\")"
      ],
      "metadata": {
        "id": "train_frozen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 7: Training Phase 2 - Fine-Tuning"
      ],
      "metadata": {
        "id": "step7_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the top layers of the base model for fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze all layers except the last 30\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate for fine-tuning\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),  # Lower LR for fine-tuning\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Count updated parameters\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "print(f\"Trainable parameters after unfreezing: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\nTraining Phase 2: Fine-Tuning\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Continue training with unfrozen layers\n",
        "history_finetuned = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=valid_generator,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nPhase 2 Fine-Tuning Complete!\")"
      ],
      "metadata": {
        "id": "fine_tuning"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 8: Plot Training History"
      ],
      "metadata": {
        "id": "step8_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine histories\n",
        "acc = history_frozen.history['accuracy'] + history_finetuned.history['accuracy']\n",
        "val_acc = history_frozen.history['val_accuracy'] + history_finetuned.history['val_accuracy']\n",
        "loss = history_frozen.history['loss'] + history_finetuned.history['loss']\n",
        "val_loss = history_frozen.history['val_loss'] + history_finetuned.history['val_loss']\n",
        "\n",
        "epochs_range = range(1, len(acc) + 1)\n",
        "phase1_end = len(history_frozen.history['accuracy'])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1.plot(epochs_range, acc, 'b-', label='Training Accuracy')\n",
        "ax1.plot(epochs_range, val_acc, 'r-', label='Validation Accuracy')\n",
        "ax1.axvline(x=phase1_end, color='g', linestyle='--', label='Fine-tuning Start')\n",
        "ax1.set_title('MobileNetV2 - Training and Validation Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "ax2.plot(epochs_range, loss, 'b-', label='Training Loss')\n",
        "ax2.plot(epochs_range, val_loss, 'r-', label='Validation Loss')\n",
        "ax2.axvline(x=phase1_end, color='g', linestyle='--', label='Fine-tuning Start')\n",
        "ax2.set_title('MobileNetV2 - Training and Validation Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/mobilenet_training_history.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal Training Accuracy: {acc[-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_acc[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "plot_history"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 9: Evaluate Model Performance"
      ],
      "metadata": {
        "id": "step9_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Load best model\n",
        "model = tf.keras.models.load_model('/content/mobilenet_best.keras')\n",
        "\n",
        "# Get predictions\n",
        "valid_generator.reset()\n",
        "predictions = model.predict(valid_generator, verbose=1)\n",
        "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
        "true_classes = valid_generator.classes\n",
        "\n",
        "# Class names\n",
        "class_names = list(valid_generator.class_indices.keys())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MOBILENETV2 CLASSIFICATION REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(true_classes, predicted_classes, target_names=class_names))\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(true_classes, predicted_classes)\n",
        "precision = precision_score(true_classes, predicted_classes)\n",
        "recall = recall_score(true_classes, predicted_classes)\n",
        "f1 = f1_score(true_classes, predicted_classes)\n",
        "\n",
        "print(\"\\nSummary Metrics:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "id": "evaluate_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('MobileNetV2 - Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig('/content/mobilenet_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "confusion_matrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 10: Measure Inference Speed"
      ],
      "metadata": {
        "id": "step10_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Create a batch of test images\n",
        "test_images = np.random.rand(100, IMG_SIZE, IMG_SIZE, 3).astype(np.float32)\n",
        "\n",
        "# Warm-up run\n",
        "_ = model.predict(test_images[:10], verbose=0)\n",
        "\n",
        "# Measure inference time\n",
        "num_runs = 5\n",
        "times = []\n",
        "\n",
        "for _ in range(num_runs):\n",
        "    start_time = time.time()\n",
        "    _ = model.predict(test_images, verbose=0)\n",
        "    elapsed = time.time() - start_time\n",
        "    times.append(elapsed)\n",
        "\n",
        "avg_time = np.mean(times)\n",
        "time_per_image = (avg_time / 100) * 1000  # ms per image\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INFERENCE SPEED (MobileNetV2)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Average batch time (100 images): {avg_time:.4f} seconds\")\n",
        "print(f\"Time per image: {time_per_image:.2f} ms\")\n",
        "print(f\"Theoretical FPS: {1000/time_per_image:.1f}\")\n",
        "\n",
        "# Get model file size\n",
        "model.save('/content/mobilenet_final.keras')\n",
        "model_size_mb = os.path.getsize('/content/mobilenet_final.keras') / (1024 * 1024)\n",
        "print(f\"\\nModel file size: {model_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "inference_speed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 11: Save Results for Comparison"
      ],
      "metadata": {
        "id": "step11_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Compile all results\n",
        "mobilenet_results = {\n",
        "    'model_name': 'MobileNetV2',\n",
        "    'model_type': 'classification',\n",
        "    'input_size': IMG_SIZE,\n",
        "    'metrics': {\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1)\n",
        "    },\n",
        "    'performance': {\n",
        "        'inference_time_ms': float(time_per_image),\n",
        "        'theoretical_fps': float(1000/time_per_image),\n",
        "        'model_size_mb': float(model_size_mb)\n",
        "    },\n",
        "    'architecture': {\n",
        "        'total_parameters': int(model.count_params()),\n",
        "        'trainable_parameters': int(trainable_params),\n",
        "        'base_model': 'MobileNetV2 (ImageNet pretrained)',\n",
        "        'custom_layers': ['GlobalAveragePooling2D', 'Dense(128)', 'Dropout(0.5)', 'Dense(1, sigmoid)']\n",
        "    },\n",
        "    'training': {\n",
        "        'epochs_phase1': len(history_frozen.history['accuracy']),\n",
        "        'epochs_phase2': len(history_finetuned.history['accuracy']),\n",
        "        'final_train_accuracy': float(acc[-1]),\n",
        "        'final_val_accuracy': float(val_acc[-1])\n",
        "    },\n",
        "    'dataset': {\n",
        "        'train_samples': train_generator.samples,\n",
        "        'valid_samples': valid_generator.samples,\n",
        "        'classes': class_names\n",
        "    },\n",
        "    'limitations': [\n",
        "        'Cannot localize crystals (no bounding boxes)',\n",
        "        'Cannot count individual crystals per image',\n",
        "        'Cannot provide per-crystal confidence scores',\n",
        "        'Cannot enable ROI-based filtering',\n",
        "        'Cannot calculate whiteness per crystal'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open('/content/mobilenet_results.json', 'w') as f:\n",
        "    json.dump(mobilenet_results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to mobilenet_results.json\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(json.dumps(mobilenet_results, indent=2))"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 12: Limitations Analysis - Why Classification is Not Enough\n",
        "\n",
        "### What MobileNetV2 Classification Provides:\n",
        "- **Single label per image**: \"pure\" or \"impure\"\n",
        "- **Overall confidence score**: 0-100%\n",
        "\n",
        "### What MobileNetV2 Classification CANNOT Provide:\n",
        "\n",
        "| Feature | Classification | Detection (YOLOv8) |\n",
        "|---------|---------------|--------------------|\n",
        "| Crystal localization | ❌ No | ✅ Bounding boxes |\n",
        "| Count crystals per image | ❌ No | ✅ Yes |\n",
        "| Per-crystal confidence | ❌ No | ✅ Yes |\n",
        "| Per-crystal whiteness | ❌ No | ✅ Yes |\n",
        "| ROI filtering | ❌ No | ✅ Yes |\n",
        "| Purity percentage | ❌ No | ✅ Yes (count-based) |\n",
        "| Real-time multi-object | ❌ No | ✅ Yes |\n",
        "\n",
        "### Why This Matters for Salt Crystal Purity Detection:\n",
        "\n",
        "1. **Counting Requirement**: The system needs to count how many pure vs impure crystals exist to calculate purity percentage.\n",
        "\n",
        "2. **Localization Requirement**: Bounding boxes enable whiteness calculation for each crystal region.\n",
        "\n",
        "3. **Batch Processing**: Detection allows tracking statistics over time (batch-level metrics).\n",
        "\n",
        "4. **Quality Control**: Per-crystal confidence filtering enables higher quality results."
      ],
      "metadata": {
        "id": "limitations_analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual demonstration of limitation\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION vs DETECTION OUTPUT COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│  SAME IMAGE - DIFFERENT MODEL OUTPUTS                       │\n",
        "├─────────────────────────────┬───────────────────────────────┤\n",
        "│   YOLOv8 DETECTION          │   MobileNetV2 CLASSIFICATION  │\n",
        "├─────────────────────────────┼───────────────────────────────┤\n",
        "│  ┌────┐  ┌────┐  ┌────┐     │                               │\n",
        "│  │pure│  │imp │  │pure│     │   Prediction: \"impure\"        │\n",
        "│  │95% │  │87% │  │92% │     │   Confidence: 67%             │\n",
        "│  └────┘  └────┘  └────┘     │                               │\n",
        "│                             │   (No location information)   │\n",
        "│  3 crystals detected        │   (No individual counts)      │\n",
        "│  2 pure, 1 impure           │   (No per-crystal metrics)    │\n",
        "│  Purity: 66.7%              │                               │\n",
        "│                             │                               │\n",
        "│  ✅ Localization            │   ❌ No localization          │\n",
        "│  ✅ Counting                │   ❌ No counting              │\n",
        "│  ✅ Per-crystal confidence  │   ❌ Only image-level         │\n",
        "│  ✅ Whiteness calculation   │   ❌ Cannot calculate         │\n",
        "│  ✅ ROI filtering possible  │   ❌ Not possible             │\n",
        "└─────────────────────────────┴───────────────────────────────┘\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nCONCLUSION:\")\n",
        "print(\"MobileNetV2 classification achieves good accuracy for individual\")\n",
        "print(\"crystal classification, but it CANNOT fulfill the requirements\")\n",
        "print(\"of the salt crystal purity detection system which needs:\")\n",
        "print(\"  - Crystal counting per image\")\n",
        "print(\"  - Bounding box coordinates for whiteness calculation\")\n",
        "print(\"  - Per-crystal confidence scores\")\n",
        "print(\"  - ROI-based filtering capability\")"
      ],
      "metadata": {
        "id": "visual_comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 13: Download Files"
      ],
      "metadata": {
        "id": "step13_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download results JSON\n",
        "print(\"Downloading mobilenet_results.json...\")\n",
        "files.download('/content/mobilenet_results.json')\n",
        "\n",
        "# Download training history plot\n",
        "print(\"Downloading training history plot...\")\n",
        "files.download('/content/mobilenet_training_history.png')\n",
        "\n",
        "# Download confusion matrix\n",
        "print(\"Downloading confusion matrix...\")\n",
        "files.download('/content/mobilenet_confusion_matrix.png')"
      ],
      "metadata": {
        "id": "download_files"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Download trained model\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading trained MobileNetV2 model...\")\n",
        "files.download('/content/mobilenet_best.keras')"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### MobileNetV2 Training Complete!\n",
        "\n",
        "This notebook trained a MobileNetV2 classification model on salt crystal images. The results are saved for comparison with YOLOv8 and ResNet50.\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Accuracy**: MobileNetV2 can achieve good classification accuracy on individual crystal crops.\n",
        "\n",
        "2. **Speed**: Efficient inference due to lightweight architecture.\n",
        "\n",
        "3. **Limitation**: Classification cannot provide the localization and counting capabilities required for the salt crystal purity detection system.\n",
        "\n",
        "### Next Steps:\n",
        "1. Run ResNet50 training notebook for comparison\n",
        "2. Use the Model Comparison notebook to visualize all results\n",
        "3. Generate final academic justification for YOLOv8 selection"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}