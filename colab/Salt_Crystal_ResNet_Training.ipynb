{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Salt Crystal Purity Classification - ResNet50\n",
        "\n",
        "This notebook trains a **ResNet50** model to classify salt crystals as **pure** or **impure** using transfer learning.\n",
        "\n",
        "## Model Architecture: ResNet50\n",
        "\n",
        "ResNet50 (Residual Network with 50 layers) is a deep convolutional neural network known for its skip connections that enable training of very deep networks.\n",
        "\n",
        "### Key Features:\n",
        "- **Residual Connections (Skip Connections)**: Solves vanishing gradient problem\n",
        "- **Bottleneck Architecture**: 1x1 → 3x3 → 1x1 convolution blocks\n",
        "- **Batch Normalization**: After each convolution\n",
        "- **Parameters**: ~25.6 million (vs 3.4M for MobileNetV2)\n",
        "- **Input Size**: 224x224x3\n",
        "- **Depth**: 50 layers (vs 53 for MobileNetV2)\n",
        "\n",
        "### Architecture Comparison\n",
        "| Feature | ResNet50 | MobileNetV2 |\n",
        "|---------|----------|-------------|\n",
        "| Parameters | 25.6M | 3.4M |\n",
        "| Key Innovation | Skip Connections | Inverted Residuals |\n",
        "| Design Goal | Accuracy | Efficiency |\n",
        "| Typical Use | Server-side | Mobile/Edge |\n",
        "\n",
        "## Before Starting\n",
        "1. Go to **Runtime > Change runtime type**\n",
        "2. Select **T4 GPU** (or any available GPU)\n",
        "3. Click **Save**"
      ],
      "metadata": {
        "id": "resnet_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 1: Check GPU & Install Dependencies"
      ],
      "metadata": {
        "id": "step1_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import tensorflow as tf\n",
        "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "metadata": {
        "id": "check_gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install/upgrade required packages\n",
        "!pip install -q pillow scikit-learn matplotlib seaborn\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Dependencies installed successfully!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 2: Mount Google Drive & Load Dataset\n",
        "\n",
        "The dataset is stored in Google Drive at `MyDrive/salt-crystal/data.zip` (same as YOLOv8 training)."
      ],
      "metadata": {
        "id": "step2_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your dataset in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/salt-crystal/data.zip'\n",
        "\n",
        "# Verify the file exists\n",
        "if os.path.exists(zip_path):\n",
        "    print(f\"\\nDataset found: {zip_path}\")\n",
        "else:\n",
        "    print(f\"\\nERROR: Dataset not found at {zip_path}\")\n",
        "    print(\"Please check the path and try again.\")\n",
        "\n",
        "# Extract the dataset\n",
        "print(\"\\nExtracting dataset...\")\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset_yolo')\n",
        "\n",
        "print(\"Dataset extracted successfully!\")\n",
        "print(\"\\nExtracted contents:\")\n",
        "!ls -la /content/dataset_yolo"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 3: Convert YOLO Detection Format to Classification Format\n",
        "\n",
        "### Problem:\n",
        "- YOLO format: Images with bounding box annotations (multiple crystals per image)\n",
        "- Classification format: Individual crystal images in class folders\n",
        "\n",
        "### Solution:\n",
        "Crop each annotated crystal from the source images and organize into `pure/` and `impure/` folders."
      ],
      "metadata": {
        "id": "step3_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "# Source paths (YOLO format)\n",
        "SOURCE_IMAGES = '/content/dataset_yolo/images'\n",
        "SOURCE_LABELS = '/content/dataset_yolo/labels'\n",
        "CLASSES_FILE = '/content/dataset_yolo/classes.txt'\n",
        "\n",
        "# Target paths (Classification format)\n",
        "TARGET_DIR = '/content/dataset_classification'\n",
        "\n",
        "# Read class names\n",
        "with open(CLASSES_FILE, 'r') as f:\n",
        "    classes = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "print(f\"Classes: {classes}\")\n",
        "print(f\"Class 0: {classes[0]}\")\n",
        "print(f\"Class 1: {classes[1]}\")\n",
        "\n",
        "# Create target directories\n",
        "for split in ['train', 'valid']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(f'{TARGET_DIR}/{split}/{cls}', exist_ok=True)\n",
        "\n",
        "print(f\"\\nTarget directory structure created at {TARGET_DIR}\")"
      ],
      "metadata": {
        "id": "setup_dirs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_crystals_from_yolo(image_dir, label_dir, output_dir, classes, target_size=224):\n",
        "    \"\"\"\n",
        "    Crop individual crystals from images using YOLO annotations.\n",
        "    \n",
        "    YOLO format: class_id x_center y_center width height (normalized 0-1)\n",
        "    \"\"\"\n",
        "    stats = {cls: 0 for cls in classes}\n",
        "    \n",
        "    image_files = [f for f in os.listdir(image_dir) \n",
        "                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "    \n",
        "    for img_file in image_files:\n",
        "        # Load image\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_width, img_height = img.size\n",
        "        \n",
        "        # Find corresponding label file\n",
        "        label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        \n",
        "        if not os.path.exists(label_path):\n",
        "            continue\n",
        "        \n",
        "        # Read annotations\n",
        "        with open(label_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        for idx, line in enumerate(lines):\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 5:\n",
        "                continue\n",
        "            \n",
        "            class_id = int(parts[0])\n",
        "            x_center = float(parts[1]) * img_width\n",
        "            y_center = float(parts[2]) * img_height\n",
        "            width = float(parts[3]) * img_width\n",
        "            height = float(parts[4]) * img_height\n",
        "            \n",
        "            # Calculate bounding box coordinates\n",
        "            x1 = max(0, int(x_center - width / 2))\n",
        "            y1 = max(0, int(y_center - height / 2))\n",
        "            x2 = min(img_width, int(x_center + width / 2))\n",
        "            y2 = min(img_height, int(y_center + height / 2))\n",
        "            \n",
        "            # Skip very small crops\n",
        "            if (x2 - x1) < 10 or (y2 - y1) < 10:\n",
        "                continue\n",
        "            \n",
        "            # Crop and resize\n",
        "            crop = img.crop((x1, y1, x2, y2))\n",
        "            crop = crop.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
        "            \n",
        "            # Save cropped crystal\n",
        "            class_name = classes[class_id]\n",
        "            output_filename = f\"{os.path.splitext(img_file)[0]}_crop{idx}.jpg\"\n",
        "            output_path = os.path.join(output_dir, class_name, output_filename)\n",
        "            crop.save(output_path, 'JPEG', quality=95)\n",
        "            \n",
        "            stats[class_name] += 1\n",
        "    \n",
        "    return stats\n",
        "\n",
        "print(\"Crystal cropping function defined.\")"
      ],
      "metadata": {
        "id": "crop_function"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's do a train/valid split of the source images\n",
        "image_files = [f for f in os.listdir(SOURCE_IMAGES) \n",
        "               if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "\n",
        "random.seed(42)  # Same seed as YOLOv8 for consistency\n",
        "random.shuffle(image_files)\n",
        "\n",
        "split_idx = int(len(image_files) * 0.9)\n",
        "train_files = set(image_files[:split_idx])\n",
        "valid_files = set(image_files[split_idx:])\n",
        "\n",
        "print(f\"Total images: {len(image_files)}\")\n",
        "print(f\"Train images: {len(train_files)}\")\n",
        "print(f\"Valid images: {len(valid_files)}\")\n",
        "\n",
        "# Create temporary directories for split\n",
        "os.makedirs('/content/temp_train/images', exist_ok=True)\n",
        "os.makedirs('/content/temp_train/labels', exist_ok=True)\n",
        "os.makedirs('/content/temp_valid/images', exist_ok=True)\n",
        "os.makedirs('/content/temp_valid/labels', exist_ok=True)\n",
        "\n",
        "# Copy files to temp directories\n",
        "for img_file in train_files:\n",
        "    shutil.copy(os.path.join(SOURCE_IMAGES, img_file), '/content/temp_train/images/')\n",
        "    label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "    label_path = os.path.join(SOURCE_LABELS, label_file)\n",
        "    if os.path.exists(label_path):\n",
        "        shutil.copy(label_path, '/content/temp_train/labels/')\n",
        "\n",
        "for img_file in valid_files:\n",
        "    shutil.copy(os.path.join(SOURCE_IMAGES, img_file), '/content/temp_valid/images/')\n",
        "    label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "    label_path = os.path.join(SOURCE_LABELS, label_file)\n",
        "    if os.path.exists(label_path):\n",
        "        shutil.copy(label_path, '/content/temp_valid/labels/')\n",
        "\n",
        "print(\"\\nFiles split into train/valid directories.\")"
      ],
      "metadata": {
        "id": "split_files"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crop crystals for training set\n",
        "print(\"Cropping training crystals...\")\n",
        "train_stats = crop_crystals_from_yolo(\n",
        "    '/content/temp_train/images',\n",
        "    '/content/temp_train/labels',\n",
        "    f'{TARGET_DIR}/train',\n",
        "    classes\n",
        ")\n",
        "print(f\"Training set: {train_stats}\")\n",
        "\n",
        "# Crop crystals for validation set\n",
        "print(\"\\nCropping validation crystals...\")\n",
        "valid_stats = crop_crystals_from_yolo(\n",
        "    '/content/temp_valid/images',\n",
        "    '/content/temp_valid/labels',\n",
        "    f'{TARGET_DIR}/valid',\n",
        "    classes\n",
        ")\n",
        "print(f\"Validation set: {valid_stats}\")\n",
        "\n",
        "# Clean up temp directories\n",
        "shutil.rmtree('/content/temp_train')\n",
        "shutil.rmtree('/content/temp_valid')\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATASET PREPARATION COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nTraining samples:\")\n",
        "for cls in classes:\n",
        "    count = len(os.listdir(f'{TARGET_DIR}/train/{cls}'))\n",
        "    print(f\"  {cls}: {count} images\")\n",
        "\n",
        "print(f\"\\nValidation samples:\")\n",
        "for cls in classes:\n",
        "    count = len(os.listdir(f'{TARGET_DIR}/valid/{cls}'))\n",
        "    print(f\"  {cls}: {count} images\")"
      ],
      "metadata": {
        "id": "crop_crystals"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 4: Create Data Generators with Augmentation"
      ],
      "metadata": {
        "id": "step4_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Training data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation data (no augmentation, only rescaling)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create generators\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{TARGET_DIR}/train',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    f'{TARGET_DIR}/valid',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"\\nClass indices: {train_generator.class_indices}\")\n",
        "print(f\"Training samples: {train_generator.samples}\")\n",
        "print(f\"Validation samples: {valid_generator.samples}\")"
      ],
      "metadata": {
        "id": "data_generators"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 5: Build ResNet50 Model with Transfer Learning"
      ],
      "metadata": {
        "id": "step5_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load ResNet50 with pretrained ImageNet weights (without top layer)\n",
        "base_model = ResNet50(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "\n",
        "# Freeze base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the complete model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(256, activation='relu'),  # Larger dense layer for ResNet\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nResNet50 Model Architecture:\")\n",
        "print(\"=\"*50)\n",
        "model.summary()\n",
        "\n",
        "# Count parameters\n",
        "total_params = model.count_params()\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "non_trainable_params = total_params - trainable_params\n",
        "\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Non-trainable parameters: {non_trainable_params:,}\")"
      ],
      "metadata": {
        "id": "build_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 6: Training Phase 1 - Frozen Base (Feature Extraction)"
      ],
      "metadata": {
        "id": "step6_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        '/content/resnet_best.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"Training Phase 1: Feature Extraction (Frozen Base)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Train with frozen base\n",
        "history_frozen = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=valid_generator,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nPhase 1 Training Complete!\")"
      ],
      "metadata": {
        "id": "train_frozen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 7: Training Phase 2 - Fine-Tuning"
      ],
      "metadata": {
        "id": "step7_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the top layers of the base model for fine-tuning\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze all layers except the last 30 (ResNet50 has 175 layers)\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile with lower learning rate for fine-tuning\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),  # Lower LR for fine-tuning\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Count updated parameters\n",
        "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "print(f\"Trainable parameters after unfreezing: {trainable_params:,}\")\n",
        "\n",
        "print(\"\\nTraining Phase 2: Fine-Tuning\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Continue training with unfrozen layers\n",
        "history_finetuned = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=valid_generator,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nPhase 2 Fine-Tuning Complete!\")"
      ],
      "metadata": {
        "id": "fine_tuning"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 8: Plot Training History"
      ],
      "metadata": {
        "id": "step8_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine histories\n",
        "acc = history_frozen.history['accuracy'] + history_finetuned.history['accuracy']\n",
        "val_acc = history_frozen.history['val_accuracy'] + history_finetuned.history['val_accuracy']\n",
        "loss = history_frozen.history['loss'] + history_finetuned.history['loss']\n",
        "val_loss = history_frozen.history['val_loss'] + history_finetuned.history['val_loss']\n",
        "\n",
        "epochs_range = range(1, len(acc) + 1)\n",
        "phase1_end = len(history_frozen.history['accuracy'])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "ax1.plot(epochs_range, acc, 'b-', label='Training Accuracy')\n",
        "ax1.plot(epochs_range, val_acc, 'r-', label='Validation Accuracy')\n",
        "ax1.axvline(x=phase1_end, color='g', linestyle='--', label='Fine-tuning Start')\n",
        "ax1.set_title('ResNet50 - Training and Validation Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "ax2.plot(epochs_range, loss, 'b-', label='Training Loss')\n",
        "ax2.plot(epochs_range, val_loss, 'r-', label='Validation Loss')\n",
        "ax2.axvline(x=phase1_end, color='g', linestyle='--', label='Fine-tuning Start')\n",
        "ax2.set_title('ResNet50 - Training and Validation Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/resnet_training_history.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal Training Accuracy: {acc[-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_acc[-1]:.4f}\")"
      ],
      "metadata": {
        "id": "plot_history"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 9: Evaluate Model Performance"
      ],
      "metadata": {
        "id": "step9_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Load best model\n",
        "model = tf.keras.models.load_model('/content/resnet_best.keras')\n",
        "\n",
        "# Get predictions\n",
        "valid_generator.reset()\n",
        "predictions = model.predict(valid_generator, verbose=1)\n",
        "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
        "true_classes = valid_generator.classes\n",
        "\n",
        "# Class names\n",
        "class_names = list(valid_generator.class_indices.keys())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESNET50 CLASSIFICATION REPORT\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(true_classes, predicted_classes, target_names=class_names))\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(true_classes, predicted_classes)\n",
        "precision = precision_score(true_classes, predicted_classes)\n",
        "recall = recall_score(true_classes, predicted_classes)\n",
        "f1 = f1_score(true_classes, predicted_classes)\n",
        "\n",
        "print(\"\\nSummary Metrics:\")\n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1-Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "id": "evaluate_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot confusion matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('ResNet50 - Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig('/content/resnet_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "confusion_matrix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 10: Measure Inference Speed"
      ],
      "metadata": {
        "id": "step10_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Create a batch of test images\n",
        "test_images = np.random.rand(100, IMG_SIZE, IMG_SIZE, 3).astype(np.float32)\n",
        "\n",
        "# Warm-up run\n",
        "_ = model.predict(test_images[:10], verbose=0)\n",
        "\n",
        "# Measure inference time\n",
        "num_runs = 5\n",
        "times = []\n",
        "\n",
        "for _ in range(num_runs):\n",
        "    start_time = time.time()\n",
        "    _ = model.predict(test_images, verbose=0)\n",
        "    elapsed = time.time() - start_time\n",
        "    times.append(elapsed)\n",
        "\n",
        "avg_time = np.mean(times)\n",
        "time_per_image = (avg_time / 100) * 1000  # ms per image\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INFERENCE SPEED (ResNet50)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Average batch time (100 images): {avg_time:.4f} seconds\")\n",
        "print(f\"Time per image: {time_per_image:.2f} ms\")\n",
        "print(f\"Theoretical FPS: {1000/time_per_image:.1f}\")\n",
        "\n",
        "# Get model file size\n",
        "model.save('/content/resnet_final.keras')\n",
        "model_size_mb = os.path.getsize('/content/resnet_final.keras') / (1024 * 1024)\n",
        "print(f\"\\nModel file size: {model_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "inference_speed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 11: Save Results for Comparison"
      ],
      "metadata": {
        "id": "step11_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Compile all results\n",
        "resnet_results = {\n",
        "    'model_name': 'ResNet50',\n",
        "    'model_type': 'classification',\n",
        "    'input_size': IMG_SIZE,\n",
        "    'metrics': {\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1_score': float(f1)\n",
        "    },\n",
        "    'performance': {\n",
        "        'inference_time_ms': float(time_per_image),\n",
        "        'theoretical_fps': float(1000/time_per_image),\n",
        "        'model_size_mb': float(model_size_mb)\n",
        "    },\n",
        "    'architecture': {\n",
        "        'total_parameters': int(model.count_params()),\n",
        "        'trainable_parameters': int(trainable_params),\n",
        "        'base_model': 'ResNet50 (ImageNet pretrained)',\n",
        "        'custom_layers': ['GlobalAveragePooling2D', 'Dense(256)', 'Dropout(0.5)', 'Dense(64)', 'Dropout(0.3)', 'Dense(1, sigmoid)'],\n",
        "        'key_innovation': 'Residual/Skip Connections',\n",
        "        'total_layers': 50\n",
        "    },\n",
        "    'training': {\n",
        "        'epochs_phase1': len(history_frozen.history['accuracy']),\n",
        "        'epochs_phase2': len(history_finetuned.history['accuracy']),\n",
        "        'final_train_accuracy': float(acc[-1]),\n",
        "        'final_val_accuracy': float(val_acc[-1])\n",
        "    },\n",
        "    'dataset': {\n",
        "        'train_samples': train_generator.samples,\n",
        "        'valid_samples': valid_generator.samples,\n",
        "        'classes': class_names\n",
        "    },\n",
        "    'limitations': [\n",
        "        'Cannot localize crystals (no bounding boxes)',\n",
        "        'Cannot count individual crystals per image',\n",
        "        'Cannot provide per-crystal confidence scores',\n",
        "        'Cannot enable ROI-based filtering',\n",
        "        'Cannot calculate whiteness per crystal',\n",
        "        'Higher computational cost than MobileNetV2',\n",
        "        'Larger model size'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open('/content/resnet_results.json', 'w') as f:\n",
        "    json.dump(resnet_results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to resnet_results.json\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(json.dumps(resnet_results, indent=2))"
      ],
      "metadata": {
        "id": "save_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 12: Architecture Comparison - ResNet50 vs MobileNetV2\n",
        "\n",
        "### ResNet50 Architecture\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│                      ResNet50 Block                         │\n",
        "├─────────────────────────────────────────────────────────────┤\n",
        "│                                                             │\n",
        "│   Input ─────────────────────────────────────┐              │\n",
        "│     │                                        │ (Skip)       │\n",
        "│     ▼                                        │              │\n",
        "│   ┌─────────────────┐                        │              │\n",
        "│   │ Conv 1x1, 64    │                        │              │\n",
        "│   │ BatchNorm, ReLU │                        │              │\n",
        "│   └────────┬────────┘                        │              │\n",
        "│            ▼                                 │              │\n",
        "│   ┌─────────────────┐                        │              │\n",
        "│   │ Conv 3x3, 64    │                        │              │\n",
        "│   │ BatchNorm, ReLU │                        │              │\n",
        "│   └────────┬────────┘                        │              │\n",
        "│            ▼                                 │              │\n",
        "│   ┌─────────────────┐                        │              │\n",
        "│   │ Conv 1x1, 256   │                        │              │\n",
        "│   │ BatchNorm       │                        │              │\n",
        "│   └────────┬────────┘                        │              │\n",
        "│            │                                 │              │\n",
        "│            └──────────── + ──────────────────┘              │\n",
        "│                         │                                   │\n",
        "│                         ▼                                   │\n",
        "│                       ReLU                                  │\n",
        "│                         │                                   │\n",
        "│                         ▼                                   │\n",
        "│                      Output                                 │\n",
        "└─────────────────────────────────────────────────────────────┘\n",
        "```\n",
        "\n",
        "### Key Insight: Skip Connections\n",
        "The skip connection (identity mapping) allows gradients to flow directly through the network, solving the vanishing gradient problem and enabling training of very deep networks.\n",
        "\n",
        "### Trade-offs vs MobileNetV2\n",
        "\n",
        "| Aspect | ResNet50 | MobileNetV2 |\n",
        "|--------|----------|-------------|\n",
        "| Accuracy | Higher potential | Good |\n",
        "| Speed | Slower | Faster |\n",
        "| Size | ~100 MB | ~14 MB |\n",
        "| Parameters | 25.6M | 3.4M |\n",
        "| Mobile Deploy | Difficult | Designed for it |"
      ],
      "metadata": {
        "id": "architecture_comparison"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 13: Limitations Analysis - Why Classification is Not Enough\n",
        "\n",
        "### Same Limitations as MobileNetV2:\n",
        "\n",
        "Despite potentially higher accuracy, ResNet50 classification **still cannot** provide:\n",
        "\n",
        "| Feature | Classification | Detection (YOLOv8) |\n",
        "|---------|---------------|--------------------|\n",
        "| Crystal localization | ❌ No | ✅ Bounding boxes |\n",
        "| Count crystals per image | ❌ No | ✅ Yes |\n",
        "| Per-crystal confidence | ❌ No | ✅ Yes |\n",
        "| Per-crystal whiteness | ❌ No | ✅ Yes |\n",
        "| ROI filtering | ❌ No | ✅ Yes |\n",
        "| Purity percentage | ❌ No | ✅ Yes (count-based) |\n",
        "\n",
        "### Additional Considerations for ResNet50:\n",
        "\n",
        "1. **Higher Computational Cost**: ~7x more parameters than MobileNetV2\n",
        "2. **Slower Inference**: Not ideal for real-time processing\n",
        "3. **Larger Model Size**: ~100 MB vs ~14 MB for MobileNetV2\n",
        "4. **Same Fundamental Limitation**: Still only classification, not detection"
      ],
      "metadata": {
        "id": "limitations_analysis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESNET50 vs YOLOV8 OUTPUT COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│  SAME IMAGE - DIFFERENT MODEL OUTPUTS                       │\n",
        "├─────────────────────────────┬───────────────────────────────┤\n",
        "│   YOLOv8 DETECTION          │   ResNet50 CLASSIFICATION     │\n",
        "├─────────────────────────────┼───────────────────────────────┤\n",
        "│  ┌────┐  ┌────┐  ┌────┐     │                               │\n",
        "│  │pure│  │imp │  │pure│     │   Prediction: \"impure\"        │\n",
        "│  │95% │  │87% │  │92% │     │   Confidence: 72%             │\n",
        "│  └────┘  └────┘  └────┘     │                               │\n",
        "│                             │   (Slightly higher accuracy   │\n",
        "│  3 crystals detected        │    but same limitation)       │\n",
        "│  2 pure, 1 impure           │                               │\n",
        "│  Purity: 66.7%              │   (No location information)   │\n",
        "│                             │   (No individual counts)      │\n",
        "│  ✅ Localization            │   ❌ No localization          │\n",
        "│  ✅ Counting                │   ❌ No counting              │\n",
        "│  ✅ Per-crystal confidence  │   ❌ Only image-level         │\n",
        "│  ✅ Whiteness calculation   │   ❌ Cannot calculate         │\n",
        "│  ✅ ROI filtering possible  │   ❌ Not possible             │\n",
        "│  ✅ Fast (15ms)             │   ❌ Slower (~40ms)           │\n",
        "└─────────────────────────────┴───────────────────────────────┘\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nCONCLUSION:\")\n",
        "print(\"ResNet50 may achieve slightly higher classification accuracy,\")\n",
        "print(\"but it has the SAME fundamental limitations as MobileNetV2:\")\n",
        "print(\"  - Cannot provide object localization\")\n",
        "print(\"  - Cannot count crystals per image\")\n",
        "print(\"  - Cannot calculate purity percentage\")\n",
        "print(\"\")\n",
        "print(\"ADDITIONALLY, ResNet50 is:\")\n",
        "print(\"  - 7x larger than MobileNetV2\")\n",
        "print(\"  - Slower inference (not suitable for real-time)\")\n",
        "print(\"  - Higher memory requirements\")"
      ],
      "metadata": {
        "id": "visual_comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Step 14: Download Files"
      ],
      "metadata": {
        "id": "step14_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download results JSON\n",
        "print(\"Downloading resnet_results.json...\")\n",
        "files.download('/content/resnet_results.json')\n",
        "\n",
        "# Download training history plot\n",
        "print(\"Downloading training history plot...\")\n",
        "files.download('/content/resnet_training_history.png')\n",
        "\n",
        "# Download confusion matrix\n",
        "print(\"Downloading confusion matrix...\")\n",
        "files.download('/content/resnet_confusion_matrix.png')"
      ],
      "metadata": {
        "id": "download_files"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Download trained model\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading trained ResNet50 model...\")\n",
        "files.download('/content/resnet_best.keras')"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### ResNet50 Training Complete!\n",
        "\n",
        "This notebook trained a ResNet50 classification model on salt crystal images. The results are saved for comparison with YOLOv8 and MobileNetV2.\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Accuracy**: ResNet50 can achieve high classification accuracy due to its deeper architecture.\n",
        "\n",
        "2. **Trade-off**: Higher accuracy comes at the cost of speed and model size.\n",
        "\n",
        "3. **Fundamental Limitation**: Like MobileNetV2, classification cannot provide the localization and counting capabilities required for the salt crystal purity detection system.\n",
        "\n",
        "### Next Steps:\n",
        "1. Use the Model Comparison notebook to visualize all results\n",
        "2. Generate final academic justification for YOLOv8 selection\n",
        "\n",
        "### Research Conclusion:\n",
        "Even with a more powerful classification model (ResNet50), the fundamental task requirements demand **object detection** capabilities that only YOLOv8 provides."
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}